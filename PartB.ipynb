{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "# Essentials\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import InceptionV3, ResNet50, InceptionResNetV2, Xception\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Flatten, Conv2D, BatchNormalization, Dropout, MaxPooling2D, Activation\n",
    "from keras.callbacks import  Callback, EarlyStopping\n",
    "\n",
    "import random\n",
    "import imageio\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "random.seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://storage.googleapis.com/wandb_datasets/nature_12K.zip\n",
    "# !unzip -q '/content/nature_12K.zip'\n",
    "\n",
    "# Define the labels for the Simpsons characters we're detecting\n",
    "class_names = {0:'Amphibia', 1:'Animalia', 2:'Arachnida',3: 'Aves',4: 'Fungi',\n",
    "              5: 'Insecta', 6:'Mammalia', 7:'Mollusca', 8:'Plantae',9: 'Reptilia'}\n",
    "\n",
    "num_classes = 10\n",
    "img_size = 128\n",
    "dir = '/content/inaturalist_12K/train'\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=45,  \n",
    "    width_shift_range=0.2, \n",
    "    height_shift_range=0.3, \n",
    "    horizontal_flip=True, \n",
    "    vertical_flip=True,  \n",
    "    validation_split=0.2 \n",
    ")\n",
    "\n",
    "\n",
    "input_shape = (224,224,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "  \"metric\": {\n",
    "      \"name\":\"val_accuracy\",\n",
    "      \"goal\": \"maximize\"\n",
    "  },\n",
    "  \"method\": \"random\",\n",
    "    \n",
    "  \"parameters\": {\n",
    "          \n",
    "        \"nn\" :{\"values\": ['Inception3','InceptionResNetV2', 'ResNet50', 'xception']},\n",
    "\n",
    "\n",
    "        \"data_aug\": {\n",
    "            \"values\": [True, False]\n",
    "        },\n",
    "        \"train_batch_size\": {\n",
    "            \"values\": [32]\n",
    "        },\n",
    "        \"dropout\": {\n",
    "            \"values\": [0, 0.2, 0.1]\n",
    "        },\n",
    "        \"fc_layer\": {\n",
    "            \"values\": [128, 256, 512]\n",
    "        },\n",
    "        \"pre_layer_train\": {\n",
    "            \"values\": [30]\n",
    "        },\n",
    "        \"epochs\":{\n",
    "            \"values\": [5,10]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(pretrained_model_name, activation_function_dense, fc_layer, dropout, pre_layer_train=None):\n",
    "    input_image_shape = (img_size, img_size, 3)\n",
    "\n",
    "    input_ = K.Input(shape = input_image_shape)\n",
    "\n",
    "    # add a pretrained model without the top dense layer\n",
    "    if pretrained_model_name == 'ResNet50':\n",
    "      pretrained_model = K.applications.ResNet50(include_top = False, weights='imagenet',input_tensor = input_)\n",
    "    elif pretrained_model_name == 'InceptionV3':\n",
    "      pretrained_model = K.applications.InceptionV3(include_top = False, weights='imagenet',input_tensor = input_)\n",
    "    elif pretrained_model_name == 'InceptionResNetV2':\n",
    "      pretrained_model = K.applications.InceptionResNetV2(include_top = False, weights='imagenet',input_tensor = input_)\n",
    "    else:\n",
    "      pretrained_model = K.applications.Xception(include_top = False, weights='imagenet',input_tensor = input_)\n",
    "    \n",
    "    #freeze all layers\n",
    "    for layer in pretrained_model.layers:\n",
    "        layer.trainable=False \n",
    "    \n",
    "    #set some of the top layers as trainable\n",
    "    if pre_layer_train:\n",
    "      for layer in pretrained_model.layers[-pre_layer_train:]:\n",
    "        layer.trainable=True\n",
    "\n",
    "    model = K.models.Sequential()\n",
    "    model.add(pretrained_model)#add pretrained model\n",
    "    model.add(Flatten()) # The flatten layer is essential to convert the feature map into a column vector\n",
    "    model.add(Dense(fc_layer, activation=activation_function_dense))#add a dense layer\n",
    "    model.add(Dropout(dropout)) # For dropout\n",
    "    model.add(Dense(10, activation=\"softmax\"))#softmax layer\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(config = sweep_config):\n",
    "\n",
    "    with wandb.init(project=\"Assignment2\", entity=\"cs6910jananiteam\", config=config):\n",
    "      config = wandb.init().config\n",
    "    \n",
    "      data_aug = config.data_aug\n",
    "      train_batch_size = config.train_batch_size\n",
    "      dropout = config.dropout\n",
    "      fc_layer = config.fc_layer\n",
    "      pre_layer_train = config.pre_layer_train\n",
    "      epochs = config.epochs\n",
    "      pre_train_model = config.nn\n",
    "      activation_function_dense = config.dense\n",
    "      \n",
    "      # Display the hyperparameters\n",
    "      # Display the hyperparameters\n",
    "      run_name = \"model_{}_aug_{}_bs_{}_drop_{}_fc_{}_fre_{}_epoc_{}\".format(pre_train_model, data_aug, train_batch_size, dropout, fc_layer, pre_layer_train, epochs )\n",
    "\n",
    "      base_model = define_model(pretrained_model_name=pre_train_model, activation_function_dense=activation_function_dense, fc_layer=fc_layer, dropout=dropout, pre_layer_train=pre_layer_train)\n",
    "\n",
    "\n",
    "      if config.pre_train_layer == 0:\n",
    "        base_model.trainable = False\n",
    "\n",
    "      stack_model = Sequential()\n",
    "      stack_model.add(base_model)\n",
    "      stack_model.add(Flatten())\n",
    "      stack_model.add(Dense(config.num_dense, activation='relu'))\n",
    "      stack_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "      \n",
    "      \n",
    "      final_model = stack_model\n",
    "      final_model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "      TrainData = datagen.flow_from_directory(\n",
    "        directory='/content/inaturalist_12K/train',\n",
    "        target_size = (224, 224),\n",
    "        batch_size=config.batch_size,\n",
    "        class_mode=\"categorical\",\n",
    "        shuffle=True,\n",
    "        subset='training',\n",
    "        seed=123)\n",
    "\n",
    "      ValidationData = datagen.flow_from_directory(\n",
    "              directory='/content/inaturalist_12K/train',\n",
    "              target_size = (224, 224),\n",
    "              batch_size=config.batch_size,\n",
    "              class_mode=\"categorical\",\n",
    "              shuffle=True,\n",
    "              subset='validation',\n",
    "              seed=123)\n",
    "      \n",
    "\n",
    "\n",
    "      if config.pre_train_layer == 0:\n",
    "\n",
    "        final_model.fit_generator(\n",
    "            TrainData,\n",
    "            steps_per_epoch = TrainData.samples // config.batch_size,\n",
    "            epochs = config.epochs,\n",
    "            verbose = 1,\n",
    "            validation_data = ValidationData,\n",
    "            validation_steps = ValidationData.samples // config.batch_size,\n",
    "            callbacks = [WandbCallback(training_data = TrainData,log_weights=(True), log_gradients=(True))]\n",
    "        )      \n",
    "\n",
    "      if config.pre_train_layer > 0:\n",
    "        base_model.trainable = True\n",
    "\n",
    "        # Freeze layers\n",
    "        freeze_point = len(base_model.layers) - config.freeze_before\n",
    "        for layer in base_model.layers[:freeze_point]:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        final_model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "\n",
    "        final_model.fit_generator(\n",
    "            TrainData,\n",
    "            steps_per_epoch = TrainData.samples // config.batch_size,\n",
    "            epochs = config.epochs,\n",
    "            verbose = 1,\n",
    "            validation_data = ValidationData,\n",
    "            validation_steps = ValidationData.samples // config.batch_size,\n",
    "            callbacks = [WandbCallback(training_data = TrainData,log_weights=(True), log_gradients=(True))]\n",
    "        )      \n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SweepId = wandb.sweep(sweep_config,project=\"Assignment2PartB\", entity=\"cs6910jananiteam\")\n",
    "wandb.agent(SweepId, function = train, count = 10)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
